{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c230231",
   "metadata": {},
   "source": [
    "# Spectrum Analysis of the Action of the Weight Matrix on the Buffer\n",
    "The goal is to assess the **action** of the weight matrix on the buffer at specific layer in the transformer. We define the action as $Y\\equiv V^TX$, with $V$ the matrix of the right singular vectors of the SVD of the weight matrix. The procedure can be summarized as follows:\n",
    "1. From a set of randomly generate prompts of vector tokens with i.i.d. entries we compute the mean of the covariance matrices of the action at a layer. We call this object Activation Covariance Matrix (ACM)\n",
    "2. Eigendecomposition of the ACM gives the principal direction of the mean of the actions. It can be shown that these eigenvectors correspond exactly to the projection of the principal directions of the mean of the prompts on the space spanned by the right singular vectors of $W$.\n",
    "3. We call those principal directions as the overlap vectors components of the Projection Matrix (PM) as each overlap vector is mathematically equivalent to the normalized projection of the principal directions of the mean of the prompts on each right singular vector. We claim that, depending on the size of the corresponding singular value, a high overlap can be interepreted as a general significant action of the weight matrix on the buffer in the specifica layer of consideration.\n",
    "4. In order to assess the statistical validity of the results we compute the Marchenko-Pastur distribution (MP) of the singular values of the weight matrix and expect to find deviations, i.e. outliers from MP bounds, corresponding higher overlaps values indicating significant action of the weight matrix on the buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438f2d3b",
   "metadata": {},
   "source": [
    "## Formalities\n",
    "\n",
    "Let $d$ be the hidden (embedding) dimension of GPT-2, here $d=768$. Let a sample be a single prompt, we run $p$ random prompt batches, each of length $m$ tokens. \n",
    "\n",
    "Let $x_{i,t}^{(s,\\ell)} \\in\\R^{n_{\\ell}}$, for $i=(1,\\dots,p),\\quad t=(1,\\dots,m)$, denote the input to sublayer $\\ell$ of block $s$. Here $n_{\\ell}$ is the dimensionality of the tokens in input to the $\\ell$-th sublayer:\n",
    "\n",
    "| Sublayer    | PyTorch module               | Input dim $n_{\\ell}$ | Output dim $m_{\\ell}$ |\n",
    "|-------------|------------------------------|-------------------|--------------------|\n",
    "| Query (q)   | `attn.c_attn[:, :d]`         | $n_q = d$       | $m_{q} = d$       |\n",
    "| Key (k)     | `attn.c_attn[:, d:2*d]`      | $n_k = d$       | $m_{k} = d$        |\n",
    "| Value (v)   | `attn.c_attn[:, 2*d:3*d]`    | $n_v = d$       | $m_{v} = d$        |\n",
    "| Attn-out (a)| `attn.c_proj`                | $n_a = d$       | $m_{a} = d$        |\n",
    "| MLP-up (u)  | `mlp.c_fc`                   | $n_u = d$       | $m_{u} = 4d$       |\n",
    "| MLP-down (d)| `mlp.c_proj`                 | $n_d = 4d$      | $m_{d} = d$        |\n",
    "\n",
    "For each $i$-th run we are able to extract the buffer to each sublayer and block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2165cf4",
   "metadata": {},
   "source": [
    "### Weight-Matrix SVD\n",
    "\n",
    "Each linear sublayer has weight matrix\n",
    "\n",
    "$$\n",
    "W^{(s,\\ell)}\\in\\R^{m_{\\ell}\\times n_{\\ell}}.\n",
    "$$\n",
    "\n",
    "We compute its SVD\n",
    "\n",
    "$$\n",
    "W^{(s,\\ell)} = U^{(s,\\ell)}\\,\\Sigma^{(s,\\ell)}\\,V^{(s,\\ell)T},\n",
    "$$\n",
    "where\n",
    "1.\t$V^{(s,\\ell)}\\in\\R^{n_{\\ell}\\times n_{\\ell}}$ has columns\n",
    "$\\{v_k^{(s,\\ell)}\\}_{k=1}^{n_{\\ell}}$, the right singular vectors.\n",
    "2.\t$\\Sigma^{(s,\\ell)}=\\mathrm{diag}(\\sigma_1\\!\\ge\\!\\sigma_2\\!\\ge\\cdots\\!\\ge\\!\\sigma_{r_{\\ell}})$ are the singular values.\n",
    "3.\t$U^{(s,\\ell)}\\in\\R^{m_{\\ell}\\times m_{\\ell}}$ has the left singular vectors.\n",
    "4.  $r_{\\ell}=\\min(m_{\\ell},n_{\\ell})$ is the numerical rank.\n",
    "\n",
    "As we only need the top $r_{\\ell} \\le n_{\\ell}$ nonzero components of the eigenvalues of the ACM, we slice out the eigenvectors with $j$-th indices beyond $r_{\\ell}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787690ee",
   "metadata": {},
   "source": [
    "### Activation Covariance Matrix (ACM)\n",
    "\n",
    "For each $(s,\\ell)$, define the sample mean across buffers as\n",
    "$$\\bar x^{(s,\\ell)} = \\frac1{p\\times m}\\sum_{i,t}x_{i,t}^{(s,\\ell)}.$$\n",
    "The Activation Covariance Matrix (ACM) of the buffers is\n",
    "$$F^{(s,\\ell)}\n",
    "=\\frac{1}{p\\times m}\\sum_{i,t}\n",
    "\\bigl(x_{i,t}^{(s,\\ell)}-\\bar x^{(s,\\ell)}\\bigr)\n",
    "\\bigl(x_{i,t}^{(s,\\ell)}-\\bar x^{(s,\\ell)}\\bigr)^T\n",
    "\\;\\in\\;\\R^{n_{\\ell}\\times n_{\\ell}}.$$\n",
    "\n",
    "Because $F^{(s,\\ell)}$ is symmetric, it admits an eigendecomposition\n",
    "$$F^{(s,\\ell)}\\,f_j^{(s,\\ell)} = \\lambda_j^{(s,\\ell)}\\,f_j^{(s,\\ell)},\n",
    "\\quad\n",
    "j=1,\\dots,n_{\\ell},$$\n",
    "with eigenvalues\n",
    "$\\lambda_1\\ge\\lambda_2\\ge\\cdots\\ge\\lambda_{n_{\\ell}}\\ge0$\n",
    "and corresponding orthonormal eigenvectors $f_j^{(s,\\ell)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ccd390",
   "metadata": {},
   "source": [
    "### Change-of-Basis Perspective\n",
    "We now ask, for each $k=1,\\dots,n_{\\ell}$, how much the $k$-th (activation) axis of the $V$ decomposition of $W$ (i.e. $v_k$) aligns with the buffer samples intrinsic directions $\\{f_j\\}$. We can visualize this by defining the Action, i.e. the coordinate transform in input space:\n",
    "$$\n",
    "Y = V^{T}\\,X, \\in \\R^{n_{\\ell}\\times m}\n",
    "$$\n",
    "where for simplicity we have dropped the $(s,\\ell)$ superscripts.\n",
    "\n",
    "1. In $Y$-coordinates, the covariance is\n",
    "$$\n",
    "\\widetilde F\n",
    "= \\mathrm{Cov}(Y)\n",
    "= V^T F V,\n",
    "$$ a similarity of $F$.\n",
    "2. Eigenpairs carry over resulting in eigendecomposition with eigenvectors\n",
    "$\n",
    "\\widetilde f_j = V^T f_j.\n",
    "$\n",
    "If\n",
    "$$\n",
    "F f_j\n",
    "= \\lambda_j\\,f_j,\n",
    "$$\n",
    "then\n",
    "$$\n",
    "\\widetilde F\\;\n",
    "\\bigl(V^T f_j\\bigr)\n",
    "= V^T F V V^T f_j\n",
    "= V^T F f_j\n",
    "= \\lambda_j\\,\\bigl(V^T f_j \\bigr).\n",
    "$$\n",
    "The eigenvectors of $\\widetilde F$ are the projection of the eigen-vectors of $F$ on the space spanned by right singular vectors in $V$.Specifically $V^Tf_j$ gives the overlaps of each $v_k$ right-vector and the $j$-th eigenvector $f_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a8ffaa",
   "metadata": {},
   "source": [
    "### Overlap Definition\n",
    "The Projection Matrix (PM) arises from the following computation:\n",
    "$$\n",
    "\\widetilde{f}_j = V^T f_j = \n",
    "\\begin{pmatrix}\n",
    "\\vec{v}_1^T \\\\\n",
    "\\vec{v}_2^T \\\\\n",
    "\\vdots \\\\\n",
    "\\vec{v}_n^T\n",
    "\\end{pmatrix}\n",
    "\\;\n",
    "\\begin{pmatrix}\n",
    "\\vec{f}_1 & \\vec{f}_2 & \\cdots & \\vec{f}_n\n",
    "\\end{pmatrix}\n",
    "\\; = \n",
    "\\begin{pmatrix}\n",
    "\\vec{v}_1^T\\vec{f}_1 & \\vec{v}_1^T\\vec{f}_2 & \\cdots & \\vec{v}_1^T\\vec{f}_n \\\\\n",
    "\\vdots               &                      &        & \\vdots               \\\\\n",
    "\\vec{v}_n^T\\vec{f}_1 & \\vec{v}_n^T\\vec{f}_2 & \\cdots & \\vec{v}_n^T\\vec{f}_n \n",
    "\\end{pmatrix}\n",
    "\\; = \n",
    "\\begin{pmatrix}\n",
    "\\vec{O}_1^T \\\\\n",
    "\\vec{O}_2^T \\\\\n",
    "\\vdots \\\\\n",
    "\\vec{O}_n^T\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "where the $\\vec{O}_k$ vectors quanify the overlap .\n",
    "\n",
    "Finally, we define the overlap as:\n",
    "$$\n",
    "O_k^{(s,\\ell)}\n",
    "=\\max_{1\\le j\\le n_{\\ell}}\\;\\bigl|\\langle v_k^{(s,\\ell)},\\,f_j^{(s,\\ell)}\\rangle\\bigr|,\n",
    "$$\n",
    "such that:\n",
    "+ If $O_k\\approx1$, then one activation‐axis $f_j$ lies essentially on $v_k$.\n",
    "+ If $O_k\\approx0$, then $v_k$ is almost orthogonal to all principal data‐axes, i.e. an unused direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5221bc0b",
   "metadata": {},
   "source": [
    "## Marchenko-Pastur of singular values of $W$\n",
    "\n",
    "We need a null model for the singular-value spectrum of a \"random\" weight matrix of the same shape as our trained $W$.  Any singular values that lie outside the theoretical MP bulk can be flagged as outliers, i.e.learned, data-driven directions.\n",
    "\n",
    "\n",
    "Let $X\\in\\R^{m\\times n}$ have i.i.d. entries with zero mean and variance $\\sigma^2$. One can form the (scaled) sample covariance\n",
    "$$\n",
    "\\begin{equation*}\n",
    "C \\;=\\;\\frac1n\\,X\\,X^T\n",
    "\\;\\in\\;\\R^{m\\times m}.\n",
    "\\end{equation*}\n",
    "$$.\n",
    "\n",
    "As $m,n\\to\\infty$ with the ratio $q = \\frac{m}{n}\\;\\;(0<q\\le1)$ fixed, the empirical eigenvalue distribution of $C$ converges to the Marchenko–Pastur law with support $\\lambda_\\pm^{(\\rm cov)}\\;=\\;\\sigma^2\\bigl(1\\pm\\sqrt{q}\\bigr)^2,$ meaning that nearly all eigenvalues of $C$ lie in\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\bigl[\\sigma^2(1-\\sqrt q)^2,\\;\\sigma^2(1+\\sqrt q)^2\\bigr]\n",
    "\\end{equation*}\n",
    "$$.\n",
    "\n",
    "The nonzero singular values of $X$ are the square-roots of the nonzero eigenvalues of $XX^T$, i.e., let $\\{\\lambda_i\\}$ be the eigenvalues of $C = \\tfrac1n\\,X X^T$,then the corresponding singular values of $X$ are $s_i(X)\\;=\\;\\sqrt{\\,n\\,\\lambda_i\\,}\\,$.\n",
    "Thus the support of the singular-value distribution of $X$ is\n",
    "$$\n",
    "\\begin{equation*}\n",
    "s_\\pm\n",
    "=\\sqrt{\\,n\\,\\lambda_\\pm^{(\\rm cov)}\\,}\n",
    "=\\sqrt{\\,n\\,\\sigma^2\\bigl(1\\pm\\sqrt q\\bigr)^2\\,}\n",
    "=\\sigma\\;\\sqrt n\\;\\bigl(1\\pm\\sqrt q\\bigr).\n",
    "\\end{equation*}\n",
    "$$.\n",
    "\n",
    "Center $W$ and compute $\\sigma^2=\\tfrac1{mn}\\sum_{i,j}W_{ij}^2$ as the empirical variance of $W_{centered}$. Set \n",
    "$$\n",
    "\\begin{equation*}\n",
    "s_- = \\sigma\\bigl|\\sqrt n - \\sqrt m\\bigr|,\\quad\n",
    "s_+ = \\sigma\\bigl(\\sqrt n + \\sqrt m\\bigr),\n",
    "\\end{equation*}\n",
    "$$\n",
    "then any empirical singular values $s_k$ outside $[s_-,s_+]$ will be outliers relative to the random baseline for that weight matrix $W$.\n",
    "\n",
    "Given that the (nonzero) eigenvalues $\\{\\lambda_i\\}$ of $C$ in the large-$m,n$ limit have density\n",
    "$$\n",
    "\\begin{equation*}\n",
    "p_C(\\lambda)\n",
    "=\\frac{1}{2\\pi\\,\\sigma^2\\,q\\,\\lambda}\n",
    "\\sqrt{(\\lambda_+^{(\\rm cov)}-\\lambda)\\,(\\lambda-\\lambda_-^{(\\rm cov)})},\n",
    "\\end{equation*}\n",
    "$$\n",
    "and that the nonzero singular values $s_i$ of $W_{centered}$ relate by\n",
    "$$\n",
    "\\begin{equation*}\n",
    "s_i = \\sqrt{\\lambda_i},\n",
    "\\end{equation*}\n",
    "$$\n",
    "so the density $p_s(s)$ satisfies\n",
    "$$\n",
    "\\begin{equation*}\n",
    "p_s(s)\\,ds \\;=\\; p_C(\\lambda)\\,d\\lambda\n",
    "\\quad\\text{with}\\quad\n",
    "\\lambda = s^2,\\quad d\\lambda = 2s\\,ds.\n",
    "\\end{equation*}\n",
    "$$\n",
    "then\n",
    "$$\n",
    "\\begin{equation*}\n",
    "p_s(s)\n",
    "= p_C(s^2)\\;\\Bigl|\\frac{d\\lambda}{ds}\\Bigr|\n",
    "= 2s\\;p_C(s^2)\n",
    "= \\frac{2s}{2\\pi\\,\\sigma^2\\,q\\,s^2}\n",
    "\\sqrt{\\bigl(\\lambda_+^{(\\rm cov)}-s^2\\bigr)\\,\\bigl(s^2-\\lambda_-^{(\\rm cov)}\\bigr)},\n",
    "\\end{equation*}\n",
    "$$.\n",
    "Then the Marchenko–Pastur distribution for each weight matrix follows:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\boxed{\n",
    "p_s(s)\n",
    "= \\frac{1}{\\pi\\,\\sigma^2\\,q\\,s}\n",
    "\\sqrt{\\bigl(s_-^2 - s^2\\bigr)\\,\\bigl(s^2- s_+^2\\bigr)},\n",
    "}\n",
    "\\end{equation*}\n",
    "$$.\n",
    "supported on $s\\in[s_-,s_+]$.\n",
    "\n",
    "By checking for right singular vectors whose associate values reside outside those boundaries we can assess in each layer which directions significantly affect the orientation of the incoming buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2831faf9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
